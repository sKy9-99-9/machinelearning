{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfba48294b1c4da479e3657c5650058",
     "grade": false,
     "grade_id": "cell-e61ca27a079c37a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Getting started with neural networks\n",
    "\n",
    "For this assignment, we're going to take a first look at neural networks. Neural networks are a model constructed of a large combination of artificial neurons. We can use logistic regression as the building block for each of these artificial neurons and create a whole network of logistic regression units.\n",
    "\n",
    "Logistic regression is not the only building block we could use, there are a lot of different types of units and connections used in neural networks. However, logistic regression is the block that was historically used in the first neural networks, and in many ways, it is the most intuitive regression unit to create such a network with. So, for this assignment, we'll build some neural networks using only logistic regression units. In the language of neural networks, we're building a fully connected neural network with sigmoid activations at every layer.\n",
    "\n",
    "This notion of combining several elemental building blocks or *modules*, in\n",
    "order to create a more complex network, is at the heart of neural networks. In\n",
    "fact, many of the recent advances in AI come from *deep* neural networks, which\n",
    "means they are the result of combining **a lot** of these modules. So, building\n",
    "the modules in such a way that we can easily stack many of them, will be\n",
    "essential.\n",
    "\n",
    "This assignment consists of three parts. In the first part you will learn more about modularity. You will re-implement logistic regression, but now in such a way that it can be used as a module in a neural network. In the second part you will use this logistic regression module to build a neural network. In the last part you will have a look at how you could implement the training of a single logistic regression module. This last part sets you up for the next module in which you will learn how to deal with the training of a complete neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6ac741e1f8ddbe4b4f2ee3c92c7218f",
     "grade": false,
     "grade_id": "cell-b1f20cd20be6c6a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Modularity (Logistic Regression)\n",
    "\n",
    "The first module we will build, as mentioned, is logistic\n",
    "regression. You've already programmed this algorithm of course, but we'll\n",
    "change the implementation to make it better fit with the idea of modularity.\n",
    "None of these changes will modify the core steps of Logistic\n",
    "regression, but they will change the way we program and mathematically\n",
    "represent the algorithm.\n",
    "\n",
    "First a brief recap of the equations required for the model function of Logistic Regression\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z &= \\mathbf{x} \\cdot \\mathbf{w} + b \\\\\n",
    "\\hat{y} &= g(z) \\\\\n",
    "g(z) &= \\frac{1}{1+e^{-z}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Next, the cost for a Logistic Regression model can be written as\n",
    "$$\n",
    "J_{\\mathbf{w},b} = - \\frac{1}{m} \\sum_{i=1}^m y^{(i)} log(\\hat{y}^{(i)}) + (1 - y^{(i)}) log(1 - \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "Then we can construct the derivative of that cost w.r.t. the bias $b$ or a particular parameter $w_j$ \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{\\mathbf{w},b}}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\\\\\n",
    "\\frac{\\partial J_{\\mathbf{w},b}}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})x_j^{(i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can also use one matrix multiplication to instead compute the whole gradient vector for\n",
    "all of $\\mathbf{w}$ in a single step\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\mathbf{w},b}}{\\partial \\mathbf{w}} = \\frac{1}{m}X^T \\cdot (\\mathbf{\\hat{y}} - \\mathbf{y}) \\\\\n",
    "$$\n",
    "\n",
    "Once we have defined the gradients for $\\mathbf{w}$ and $b$, we can use gradient descent to incrementally update $\\mathbf{w}$ and $b$ as usual, in order to lower the cost and thus improve the predictions for the training data.\n",
    "\n",
    "The changes we'll make to Logistic Regression are to ensure we can easily\n",
    "combine many different modules of Logistic Regression together. The two\n",
    "main changes for this will be:\n",
    "\n",
    "1. Viewing every step of the algorithm as part of a computational graph, even\n",
    "a simple step like adding the bias. This might seem like a strange step at\n",
    "first, but it will make trying to learn the parameters of all these modules a\n",
    "lot easier.\n",
    "<br><br>\n",
    "2. Implementing the algorithm using *Object Oriented* programming, making it\n",
    "possible to create several *instances* of the algorithm, which can then easily\n",
    "be stacked and linked together, creating a large network of these smaller\n",
    "self-contained modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dab14c1a10f4821eebbbacdd8ccdfd57",
     "grade": false,
     "grade_id": "cell-2435918f042a4ab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 1: computational graphs\n",
    "\n",
    "\n",
    "#### Computational graphs in general\n",
    "\n",
    "A convenient way to represent the mathematics in neural networks is using computational graphs. It allows you to represent a neural network as data flowing through a network of computation. Especially when we're going to look at backpropagation (i.e. learning), this is particularly useful. We will already start using computational graphs here, to get used to the notation.\n",
    "\n",
    "The following graph represents the computation $c= a + b$\n",
    "\n",
    "<img src=\"src/cg1.svg\" width=\"20%\">\n",
    "\n",
    "The computation $+$ is represented as a node where the data from variables $a$ and $b$ flow into.\n",
    "\n",
    "So if the values of $a$ and $b$ are $2$ and $1$, respectively the data-flow through the computational graph would look like this:\n",
    "\n",
    "<img src=\"src/cg2.svg\" width=\"20%\">\n",
    "\n",
    "Let's have a look at a second, more complex example, $c = ln(ab + 2a^2)$:\n",
    "\n",
    "<img src=\"src/cg3.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce240c955eddd3b3020c303583f6c9ad",
     "grade": false,
     "grade_id": "cell-bd91c429eb6aeae8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Logistic regression as computational graph\n",
    "\n",
    "Similarly we can represent logistic regression as a computational graph. We adopt the convention to represent the non vectorized version of the equations for the computational graph. So we will assume a single input vector $x$ and a single output value $y$.\n",
    "\n",
    "The computational graph for logistic regression looks like this:\n",
    "\n",
    "<img src=\"src/Logistic_Regression_wb.png\" width=\"40%\">\n",
    "\n",
    "And we represent the gradients by dashed arrows going in the opposite direction:\n",
    "\n",
    "<img src=\"src/Logistic_Regression_gradients_wb.png\" width=\"40%\">\n",
    "\n",
    "With the relatively low complexity of logistic regression, this representation might not feel very useful yet. But it will start to make more sense once we move on to neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f622e781822b404f3516b16024edfeb",
     "grade": false,
     "grade_id": "cell-0f994012c73af2df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 2: implemented as a Python class\n",
    "\n",
    "#### Gradient descent algorithms\n",
    "\n",
    "Before we implement logistic regression as a class, it is good to think about its specifications. All gradient descent algorithms (e.g., linear and logistic regression) follow the same pattern: \n",
    "\n",
    "- They all predict some output $y$ based on some input $X$ (the _forward_ pass).\n",
    "- There is some cost function (loss) that allows us to quantify how good our current predictions are.\n",
    "- They all compute some gradient that tells us how we should change the parameters of the model to give better predictions next time (the _backward_ pass).\n",
    "- There is some form of gradient descent in which we take a step in the opposite direction of the gradient.\n",
    "\n",
    "#### Using classes\n",
    "\n",
    "In the previous modules you implemented these steps for both linear and logistic regression using separate variables and functions. This made running the algorithm and managing the data quite messy. The idea of using a class is that we can bundle all of this together and make running the descent algorithm much cleaner. \n",
    "\n",
    "Before we implement the class, let's have a look at how it is *intended to be used*. What we would like is that (once the class is defined) we can create a new logistic regression model and set the initial values of the learning parameters ($\\mathbf{w}$ and $b$) by simply creating a new instance. For example, if we want to have a logistic model with 4 inputs:\n",
    "\n",
    "    my_model = Logistic(4)\n",
    "\n",
    "Then, if we want do a step of the gradient descent algorithm for some input `X` and some given output `y`, we want to be able to do something like this:\n",
    "\n",
    "    # predict the output for X\n",
    "    my_model.forward(X)\n",
    "    \n",
    "    # compute the gradients\n",
    "    my_model.backward(y)\n",
    "    \n",
    "    # update the learning parameter w and b, using a learning rate alpha\n",
    "    my_model.step(alpha)\n",
    "\n",
    "Note that nowhere above you see any variables representing the learning parameters or the gradients. This is the crucial part: they are encapsulated inside the class. So once the class is defined we don't have to think about them anymore. Those details will be abstracted away by our design. \n",
    "\n",
    "The function `optimize()` is the function that will run the gradient descent given the model that you will implement. The function `optimize()` is already implemented a little bit further down. Have a look at it, you will see it implements the logistic algorithm exactly as described here. However, it will not work yet, because the implementation of the class `Logistic` is not finished yet. This is left up to you to finish.\n",
    "\n",
    "#### Class specification\n",
    "\n",
    "As we established, all gradient descent algorithms are very similar and so the classes implementing them will all look very similar. We can rely on the previously mentioned commonalities to define a general template for any of those algorithms. We will agree that any class that represents a gradient descent module, will implement the following methods:\n",
    "\n",
    "- `__init__`, set initial learning parameter.\n",
    "- `forward`, given some input $x$ predict an output $\\hat{y}$.\n",
    "- `backward`, compute all gradients of the parameters (given the output $\\hat{y}$ computed in `forward`, and the target value $y$).\n",
    "- `step`, update all parameters (based on the gradients computed in `backward`, and a learning rate `alpha`).\n",
    "- `cost`, compute the cost (given the output $\\hat{y}$ computed in `forward`, and the target value $y$).\n",
    "\n",
    "By observing this template we should be able to apply the gradient descent algorithm to any module by repeatedly calling the methods `forward`, `backward` and `step` in succession, until we reach convergence (whatever our criterion for convergence might be).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73130bd38557193d154a0270faca6552",
     "grade": false,
     "grade_id": "cell-f5a3c25f51774e0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 1\n",
    "\n",
    "Now we're going to re-implement logistic regression as a class. We've made a start with the implementation, but the class `Logistic` below is not completely finished. Your task is to complete the `TODO`'s. Of course, you can re-use some code from the logistic regression assignment from last week, but bear in mind that there are some changes in how things are represented in this *Object Oriented* version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7d01264345bf709839d8a2bbad902c8",
     "grade": false,
     "grade_id": "cell-c05cd33d2759806c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from notebook_checker import start_checks\n",
    "# Start automatic globals checks\n",
    "%start_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eeabc798043c2c93a39994c85542df8a",
     "grade": true,
     "grade_id": "cell-949db3489a20fe97",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Logistic():\n",
    "    def __init__(self, s_in):\n",
    "        \"\"\" Set initial values of parameters. \"\"\"\n",
    "        # the parameters (updated in step)\n",
    "        self.w = np.zeros(s_in)\n",
    "        self.b = 0\n",
    "        \n",
    "        # the input and output values (set in forward)\n",
    "        self.X = None\n",
    "        self.y_hat = None\n",
    "        \n",
    "        # the gradient values (set in backward)\n",
    "        self.d_w = None\n",
    "        self.d_b = None\n",
    "  \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction, y_hat, based on self.w, self.b and x.\"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        # predict (compute self.z and self.y_hat)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, y):\n",
    "        \"\"\" Compute gradients self.d_w and self.d_b, based on self.y_hat and y.\"\"\"\n",
    "        self.y = y\n",
    "        \n",
    "        # compute gradients (self.d_w and self.d_b)\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    def step(self, alpha = 0.1):\n",
    "        \"\"\" Update self.w and self.b based on self.d_w, self.d_b, and alpha.\"\"\"\n",
    "        # single gradient descent step (update self.w and self.b)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def cost(self, y = None):\n",
    "        \"\"\" Compute cost, based on prediction, self.y_hat, and target: y (or self.y).\"\"\"\n",
    "        if not y:\n",
    "            y = self.y\n",
    "            \n",
    "        # return cost based on:\n",
    "        # - the predicted output (self.y_hat) \n",
    "        # - and the actual values (y or self.y)\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bfd212942c1b50cff31843020a44a82",
     "grade": false,
     "grade_id": "cell-0988c34a857ee1e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The function `optimize` below runs the gradient descent algorithm. If you implemented the methods above correctly, this function should work for the `Logistic` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08a76763ed9e999c86571769f1007468",
     "grade": false,
     "grade_id": "cell-478f0a7ed43cc59c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize(model, X, y, alpha, iterations = 500):\n",
    "    \"\"\"Apply gradient descent for `iterations` number of steps to any learning model that\n",
    "       implements the correct functions, i.e. forward(), backward(), step() and cost().\"\"\"\n",
    "    costs = []\n",
    "    for i in tqdm(range(iterations)):\n",
    "        # descent\n",
    "\n",
    "        model.forward(X)\n",
    "        model.backward(y)\n",
    "        model.step(alpha)\n",
    "        \n",
    "        # keep track of costs\n",
    "        costs.append(model.cost())\n",
    "        \n",
    "        # check for divergence (alpha too big)\n",
    "        if len(costs) >= 2 and (costs[-2] - costs[-1]) < 0:\n",
    "            print(f'Diverging at iteration {len(costs)}')\n",
    "            return costs  \n",
    "    return costs\n",
    "\n",
    "def confusion_matrix(p, y):\n",
    "    return np.matmul(np.vstack((p, 1 - p)), np.vstack((y, 1 - y)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "528897cbab28740a5a941efab4294127",
     "grade": false,
     "grade_id": "cell-e431931347410616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test your solution\n",
    "Let's start by loading the titanic data again. This time we start with a version of this dataset that we've already cleaned up and is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "618e1a5c454f7a22d8276a5fa8a4cac9",
     "grade": false,
     "grade_id": "cell-be1b144451e8688f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def round_output(x):\n",
    "    return (x >= 0.5).astype(int)\n",
    "\n",
    "data = pd.read_csv('data/clean_titanic.csv', index_col=0)\n",
    "y = data['Survived'].to_numpy()\n",
    "X = data.drop('Survived', axis=1).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3,  random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f65529d5c9e5760a1e3acab7e71e21c2",
     "grade": false,
     "grade_id": "cell-c4b3154a46240e5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code below runs the gradient descent and shows the confusion matrix.\n",
    "\n",
    "If all went well, the confusion matrix should look the same to the one you found in the previous assignment for logistic regression. The values should be close to this:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "& y=1 & y=0 \\\\\n",
    "\\hat{y}=1 & 69 & 21 \\\\\n",
    "\\hat{y}=0 & 23 & 155\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b91e30b71d02dbc66a616c051df8dfc",
     "grade": false,
     "grade_id": "cell-185b233c22dac50a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# train model and predict\n",
    "logistic_model = Logistic(X_train.shape[1])\n",
    "optimize(logistic_model, X_train, y_train, 0.2)\n",
    "predictions = round_output(logistic_model.forward(X_test))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9f8e64906c6068ccf00fddd5c9e0f5f",
     "grade": false,
     "grade_id": "cell-67e1d9dbff5fec1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Neural networks\n",
    "\n",
    "### Logistic layer\n",
    "\n",
    "As you have learned from Andrew's videos, classic neural networks consist of multiple logistic layers. Each layer is essentially Logistic Regression, but instead of one single output, a logistic layer can have multiple outputs. And those outputs serve as the inputs for the next layer.\n",
    "\n",
    "#### Intuitive solution\n",
    "\n",
    "So, let's say we want to create a logistic layer that outputs $o$ values labeled $y_1, y_2, \\ldots y_o$. One way you could theoretically accomplish this is by just creating $o$ different logistic regression units with $o$ individual $\\mathbf{w}$ parameters, and $o$ individual $b$ parameters, that all operate on the input $x$, like so:\n",
    "\n",
    "<img src=\"src/Multi_Logistic_Regression_wb.png\" width=\"50%\">\n",
    "\n",
    "Where we could create the output by composing all the outputs of the individual units into one vector: \n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\begin{pmatrix} \\hat{y}_1 & \\hat{y}_2 & \\cdots & \\hat{y}_o \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Efficient solution\n",
    "\n",
    "This makes intuitive sense but it is not particularly elegant, and it will not be very efficient if you would implement it this way. Generally we always try to express everything as much as possible as a single matrix operation. This makes use of the hardware you use for the computation much more efficiently. \n",
    "\n",
    "We can rewrite the above graph by creating a single weight matrix combining all the weights $\\mathbf{w}_i$ and a vector $b$ containing all the biases $b_i$:\n",
    "\n",
    "$$\n",
    "W = \n",
    "\\begin{pmatrix} \n",
    "-\\mathbf{w}_1 - \\\\ \n",
    "-\\mathbf{w}_2 - \\\\ \n",
    "\\vdots \\\\\n",
    "-\\mathbf{w}_o -  \n",
    "\\end{pmatrix}\n",
    ",\n",
    "b = \n",
    "\\begin{pmatrix} \n",
    "b_1 \\\\ \n",
    "b_2 \\\\ \n",
    "\\vdots \\\\\n",
    "b_o\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The resulting computational graph:\n",
    "\n",
    "<img src=\"src/cg4.svg\" width=\"40%\">\n",
    "\n",
    "In the form of equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z &= W \\cdot x + b\\\\\n",
    "\\hat{y} &= g(z)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Which are essentially the same equations as for logistic regression. But, instead of the vector $\\mathbf{w}$, we have an $o\\times n$ weight matrix called $W$ (for an input containing $n$ features and $o$ output values). And the bias $b$ is not a scalar value, but a vector of size $o$.\n",
    "\n",
    "> You've seen two possible solutions: The composed logistic regression units and the representation as a single mathematical system. Verify for yourself that they are mathematically equivalent!\n",
    "\n",
    "> Note that Andrew uses slightly different conventions here: He uses the symbol $\\Theta$ for the weight matrix. And he doesn't use a separate $b$ vector, but instead relies on the trick of augmenting the input vector with $x_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3548465b47ec3d8a4b6e8379f63d2ab4",
     "grade": false,
     "grade_id": "cell-eb115918f19a19bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic layer vectorized\n",
    "\n",
    "We have to pay attention when we want to compute multiple samples at once. When $X$ is an $m \\times n$ matrix ($n$ features, $m$ samples) and we want to compute $o$ different outputs, we'll have to compute the matrix $\\hat{Y}$, which then will be of dimension $m \\times o$, all at once:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = g(Z)\n",
    "$$\n",
    "\n",
    "Here $Z$ will be the input of the Logistic function, so this must be a matrix of those same dimensions $m \\times o$ ($o$ outputs, $m$ samples). To compute $Z$ we'll need $b$, the vector containing $o$ bias terms and $W$, which is an $o \\times n$ matrix of weights. The main difference with simple Logistic Regression is that you need to transpose the weight matrix in order to keep the dimensions correct for $Z$ in the final output:\n",
    "\n",
    "$$\n",
    "Z = X \\cdot W^{T} \\oplus b\n",
    "$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "So if we have a logistic layer with 2 inputs and 3 outputs, with the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z &= X \\cdot W^{T} \\oplus b\\\\\n",
    "\\hat{Y} &= g(Z)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The values are represented by the following matrices (for an input containing $m$ samples):\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} \\\\ \n",
    "x_{2,1} & x_{2,2} \\\\ \n",
    "\\vdots & \\vdots \\\\\n",
    "x_{m,1} & x_{m,2} \\\\ \n",
    "\\end{pmatrix}\n",
    ",\n",
    "\\hat{Y} = \n",
    "\\begin{pmatrix}\n",
    "\\hat{y}_{1,1} & \\hat{y}_{1,2} & \\hat{y}_{1,3} \\\\ \n",
    "\\hat{y}_{2,1} & \\hat{y}_{2,2} & \\hat{y}_{2,3} \\\\ \n",
    "\\vdots & \\vdots  & \\vdots \\\\\n",
    "\\hat{y}_{m,1} & \\hat{y}_{m,2} & \\hat{y}_{m,3}\n",
    "\\end{pmatrix}\n",
    ",\n",
    "W = \n",
    "\\begin{pmatrix}\n",
    "W_{1,1} & W_{1,2} \\\\ \n",
    "W_{2,1} & W_{2,2} \\\\ \n",
    "W_{3,1} & W_{3,2} \n",
    "\\end{pmatrix}\n",
    ",\n",
    "b = \n",
    "\\begin{pmatrix}\n",
    "b_{1} & b_{2} & b_{3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "If you enter those values into the equations, you will get the following output:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \n",
    "\\begin{pmatrix}\n",
    "g(W_{1,1}x_{1,1} + W_{1,2}x_{1,2} + b_1) & g(W_{2,1}x_{1,1} + W_{2,2}x_{1,2} + b_2) & g(W_{3,1}x_{1,1} + W_{3,2}x_{1,2} + b_3)\\\\ \n",
    "g(W_{1,1}x_{2,1} + W_{1,2}x_{2,2} + b_1) & g(W_{2,1}x_{2,1} + W_{2,2}x_{2,2} + b_2) & g(W_{3,1}x_{2,1} + W_{3,2}x_{2,2} + b_3) \\\\ \n",
    "\\vdots & \\vdots  & \\vdots \\\\\n",
    "g(W_{1,1}x_{3,1} + W_{1,2}x_{3,2} + b_1) & g(W_{2,1}x_{3,1} + W_{2,2}x_{3,2} + b_2) & g(W_{3,1}x_{3,1} + W_{3,2}x_{3,2} + b_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Verify this for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6f1bbc37209e6635ae3bc3b58ca3909",
     "grade": false,
     "grade_id": "cell-b8d7949f80c6babb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2\n",
    "\n",
    "Implement the class `LogisticLayer` below. You will only have to implement the `init` and the `forward` method. Note that the `forward` method should be very similar to the one of logistic regression. Following the mathematics outlined above, the forward method should implement the following computational graph:\n",
    "\n",
    "<img src=\"src/cg5.svg\" width=\"40%\">\n",
    "\n",
    "The `init` method should set the $W$ and $b$ parameters (`self.W` and `self.b`) as `np.array`'s of the right dimensions, as specified by `s_in` (the input size) and `s_out` (the output size). Both $W$ and $b$ should start out filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42f83dc25be11b6f4eac3ff32e145b02",
     "grade": true,
     "grade_id": "cell-f502a850337d44e3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" Set initial values of parameters. \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def manually_set_weights(self, W, b):\n",
    "        \"\"\" Set weights manually. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert self.W.shape == W.shape, \"W: wrong shape\"\n",
    "        assert self.b.shape == b.shape, \"b: wrong shape\"\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction, y_hat, based on self.W, self.b and X.\"\"\"\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bafb68cf2a0e5ed593bd3c602a33e64",
     "grade": false,
     "grade_id": "cell-728a23770532ff4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb7453fbd1c9f215953984d762904e20",
     "grade": true,
     "grade_id": "cell-692b8ef24379540b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test Logistic Layer\n",
    "testLL = LogisticLayer(3,2)\n",
    "\n",
    "# some arbitrary test values\n",
    "testX = np.array([[-1,  0,  1]])\n",
    "testY = np.array([[0.26894142, 0.73105858]])\n",
    "\n",
    "# set weights\n",
    "testLL.manually_set_weights(np.array([[1, 1, 1], [0, 0, 0]]), np.array([-1, 1]))\n",
    "\n",
    "np.testing.assert_allclose(testLL.forward(testX), testY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbb179094ea8d06e0cfb8cb22d4c59f3",
     "grade": false,
     "grade_id": "cell-86c44b2427810801",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Representing logic functions\n",
    "\n",
    "The function above can now produce a network output for any $o$ number of neurons, using a single matrix multiplication, with just those slightly modified Logistic regression functions from before. The only thing needed to produce $o$ outputs is a weight matrix $W$ and bias vector $b$ of the correct dimensions, and then the result should be a hypothesis matrix $\\hat{Y}$ with dimensions $m \\times o$.\n",
    "\n",
    "### Assignment 3\n",
    "\n",
    "In the theory videos, Andrew describes how to encode the boolean *AND* function in neural network weights. This is of course a not very realistic toy example. You wouldn't use neural networks for logic operations, but it can be interesting to look at this example to get a feel for how neural networks work. \n",
    "\n",
    "Recreate Andrew's example using the `LogisticLayer` class to build a really simple network called `andLL`, with 2 inputs and 1 output (i.e. just basic logistic regression). Andrew's example assumes that the bias is part of the $\\Theta$ matrix, whereas we split that up into a separate weight matrix ($W$) and bias vector ($b$). So your solution will look a bit different from the one Andrew proposes.\n",
    "\n",
    "- Make sure $W$ is a matrix of the correct shape and has the correct values for each of the required $W_{ji}$.\n",
    "- Make sure the $b$ is a vector of the correct size.\n",
    "- The network output should be a *column* vector of 4 outputs, corresponding to the logical *AND* of each of the 4 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a2d6935448bd9c6e7bb8f3cab846439",
     "grade": true,
     "grade_id": "cell-f814f9b203f7773a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(round_output(andLL.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42283bde7365a903c1b0dd22fd033a02",
     "grade": false,
     "grade_id": "cell-dae0683aeb5cbc98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1693f98b60661c65680ee9c7210a0a28",
     "grade": true,
     "grade_id": "cell-4b8a17ad79da5bf6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(andLL.forward(X)), andY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5a7b46e3ab8ed54a84c161f30bc5331",
     "grade": false,
     "grade_id": "cell-d69d93a1e586d8d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 4\n",
    "\n",
    "In the same spirit we can encode some other logical operations. Encode the logical operations *OR* and *NAND* below with neural networks `orLL` and `nandLL` respectively. If you're not sure of their definitions, the expected output is provided.\n",
    "\n",
    "Hint: Think how the input corresponds to the output and work your way back. For which output do you need $z$ to be (very) high, and for which output do you need $z$ to be very low. How does this correspond to the input? How can you manipulate $W$ and $b$ to achieve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "814ba2c03b44480327c8f8c59650f149",
     "grade": true,
     "grade_id": "cell-f54b68efea82e510",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "orY = np.array([[0], [1], [1], [1]])\n",
    "nandY = np.array([[1], [1], [1], [0]])\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bd3ab1b9dc5874910718fecf1c6fa5e",
     "grade": true,
     "grade_id": "cell-8a88ac25456bba7c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(orLL.forward(X)), orY)\n",
    "np.testing.assert_array_equal(round_output(nandLL.forward(X)), nandY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0135499884af313d18172b948d2e013b",
     "grade": false,
     "grade_id": "cell-701963f9a602cb60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 5\n",
    "\n",
    "Of course, a logistic layer can have more than one output. Create a new logistic layer called `or_nandLL` that has two outputs, corresponding the *OR* and *NAND* values for every input respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b35c6b343133ea7bac4c85430d208c73",
     "grade": true,
     "grade_id": "cell-766985420c457b0c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "or_nandY = np.array([[0, 1], [1, 1], [1, 1], [1, 0]])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0f06eeddf4d750d87d6ab69a02cb845",
     "grade": false,
     "grade_id": "cell-f570f3425a0a6481",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d115565c019e63e2ae4c659d2da8ab8",
     "grade": true,
     "grade_id": "cell-7db7ad00b8aa8e6b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(or_nandLL.forward(X)), or_nandY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cc6f6ad72c60ea0de82f9b20e6a0d1b",
     "grade": false,
     "grade_id": "cell-0fedcb18d1a1d204",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6\n",
    "\n",
    "Last one: Try to make a single logistic layer that models the exclusive or (XOR). If you can't get this to work, then instead explain why in the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "015f2ebc735136323e3eaa81318a3621",
     "grade": true,
     "grade_id": "cell-aaf6897d957ba8c7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(round_output(xorLL.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbf537351dc977db15afc7027d267260",
     "grade": false,
     "grade_id": "cell-c990d11e19681aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q1. If you managed to get a logistic layer to output XOR, explain how you determined the weights. If not, explain what problem you encountered and why this cannot be be solved with a logistic layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c39af39584d875884fda626f72dfa8f",
     "grade": true,
     "grade_id": "cell-495541848a01ce8a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b3e29bcd9bcd69b9e01072061b167cb",
     "grade": false,
     "grade_id": "cell-dcc0550e49bea186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-layer neural networks\n",
    "\n",
    "For some problems, you need to add a *hidden* layer to the neural network. This is where the real expressive power of neural networks lies; combining layers of the network to represent *non-linear* features and solve complex, non-linear functions.\n",
    "\n",
    "A neural network with 2 layers can already learn *many* more functions than a single layer network. In general, you can stack $n$ different layers together and the basic principle does not change: The output matrix for one layer becomes the input matrix for the next layer, repeated for as many layers as there are in the network.\n",
    "\n",
    "#### Computational graph\n",
    "\n",
    "The computational graph for a 2 layer network looks as follows:\n",
    "\n",
    "<img src=\"src/Multi layer NN.svg\" width=\"70%\">\n",
    "\n",
    "In general, the computational graph for a single layer looks like this:\n",
    "\n",
    "<img src=\"src/Single layer NN.svg\" width=\"35%\">\n",
    "\n",
    "#### The math\n",
    "\n",
    "Written as equations:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_{i} &= A_i \\cdot W_{i}^T \\oplus b \\\\\n",
    "A_{i+1} &= g(Z_{i})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where the input of layer $i$ is $A_i$ and the output is $A_{i+1}$. For the first layer ($i = 0$) the input is our data $A_{0} = X$. For the last layer (in the example above $i = 1$) the output is the value we're predicting $A_{2} = \\hat{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d590bd2ea6fe57c7f29faf32105a9333",
     "grade": false,
     "grade_id": "cell-ef45de451451d89a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "We can now create a new class called `NN` containing a multilayer neural network. Every layer is a `LogisticLayer` object. The `NN` class follows the same template as `LogisticLayer` did, and so in principle this class should also be implementing all the relevant methods (`__init__`, `forward`, `backward`, `step`, and `cost`). However, for now we will limit ourselves to only the `__init__` and `forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04603720247717ce17a322c70ba44107",
     "grade": false,
     "grade_id": "cell-60947c0262ef1fe8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 7\n",
    "\n",
    "The `NN` class is partially implemented. It contains an `init` method that, given a list with layer dimensions, will create a list with matching `LogisticLayer`objects.\n",
    "\n",
    "Your goal is to add a `forward` method to this class. It should not require you to write any new mathematical code. You should be able to rely on the `forward` of the logistic layers in `self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54c1e21b73f197268c7e0c55ae0c2858",
     "grade": true,
     "grade_id": "cell-ce857e0ba13db728",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, layer_sizes = [2,2,1]):\n",
    "        \"\"\" Set initial layers. \"\"\"\n",
    "        self.layers = []\n",
    "        for s_in, s_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(LogisticLayer(s_in, s_out))\n",
    "            \n",
    "    def manually_set_weights(self, Ws, bs):\n",
    "        \"\"\" Provide list of weight matrices Ws and list of bias vectors bs. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert len(Ws)==len(self.layers), \"Ws: wrong length\"\n",
    "        assert len(bs)==len(self.layers), \"bs: wrong length\"\n",
    "        \n",
    "        # Loop over each layer in the list of layers and set the weights for that layer manually\n",
    "        # using the provided lists of W's and b's.\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].manually_set_weights(Ws[i], bs[i])\n",
    "        \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7acac3cde5258b6f29c3b37c9574a0b0",
     "grade": false,
     "grade_id": "cell-6b339d15210e26be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d3ac9c672ae3019f4e48f782b2e1e03",
     "grade": true,
     "grade_id": "cell-91fff74d8978ae9f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testNN = NN(layer_sizes = (3,2,2))\n",
    "\n",
    "# layer 0: 3(+1) -> 2\n",
    "testNN.manually_set_weights([np.array([[0.5, 1.0, 1.5], [-0.5, -1.0, -1.5]]), \n",
    "                             np.array([[1, 0], [0.5, 0.5]])],\n",
    "                            [np.array([-1, 1]),\n",
    "                             np.array([0, 1])])\n",
    "\n",
    "# output\n",
    "result = testNN.forward(np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]))\n",
    "answer = np.array([[0.56683301, 0.81757448],\n",
    "                   [0.65077768, 0.81757448],\n",
    "                   [0.62245933, 0.81757448],\n",
    "                   [0.69372123, 0.81757448],\n",
    "                   [0.59327981, 0.81757448],\n",
    "                   [0.67503753, 0.81757448],\n",
    "                   [0.65077768, 0.81757448],\n",
    "                   [0.70698737, 0.81757448]])\n",
    "\n",
    "np.testing.assert_allclose(result, answer)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9521320588cfc46014a70b74f0b85dd7",
     "grade": false,
     "grade_id": "cell-194a50cbd4b4cc36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 8\n",
    "\n",
    "Andrew Ng also discusses in detail how to encode the solution for the *XNOR* problem in neural network weights. The network should have 2 inputs, 2 hidden nodes and 1 output, and the necessary added bias nodes for the input and hidden layer.\n",
    "\n",
    "The same is true for the *XOR* network. Define a neural network called `xorNN` that produces the correct *XOR* output.\n",
    "\n",
    "> Hint: you can represent a *XOR* b as a combination of other logic ports: (a *NAND* b) *AND* (a *OR* b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29066aa4a84df0c9f84657184c023e47",
     "grade": true,
     "grade_id": "cell-cbe451bd8111af63",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdbcdc30e6a9745d5100c1c85222dbf0",
     "grade": false,
     "grade_id": "cell-1c76b2353c92b8e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee65e10198eb5dd3b335c2da7fbe2c71",
     "grade": true,
     "grade_id": "cell-3b5faaefbb75c43c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(xorNN.forward(X)), xorY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68a7271c74ddae1e3b40047f97e1c5dd",
     "grade": false,
     "grade_id": "cell-738f73c0f622a8f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Classifying handwritten digits\n",
    "\n",
    "This principle is at the *core* of neural networks; adding layers together can solve *much* more complicated problems than just using one layer. For the last part of this assignment, we'll try this algorithm on something a little more interesting than boolean functions, digit recognition.\n",
    "\n",
    "The data for this problem can be found in the file `digits123.csv`. Each row contains 65 values, where the first 64 are grayscale pixel values, and the last value is the class label, corresponding to the digit being shown. The grayscale values are integers ranging from 1 to 16, and using some reshaping, can be reconstructed back into a low-resolution *8x8* image.\n",
    "\n",
    "Below is the code to show the image for one digit, i.e. the input for one sample in the data set. Run the code to see the image. Currently, this is the sample at index 200 in the data set, but you can also change this index to display different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = np.loadtxt('data/digits123.csv', delimiter=',', dtype=int)\n",
    "\n",
    "def display_digit(i, digits):\n",
    "    digit_sample = np.ones((8,8))*16 - np.reshape(digits[i, :-1], (8, 8))\n",
    "    plt.imshow(digit_sample, cmap='gray', vmax=16)\n",
    "    plt.show()\n",
    "    print(\"The label for this digit was:\", digits[i, -1])\n",
    "\n",
    "display_digit(200, digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60331463378b0e09dbe2db5af21bc0a4",
     "grade": false,
     "grade_id": "cell-99cf5a79f07ec229",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Digit recognition solution\n",
    "\n",
    "Recognizing these very low-resolution digits, based on the individual pixel values, is probably not a task with a simple linear decision boundary. However, it is actually solvable with just one hidden layer!\n",
    "\n",
    "Learning the weights of a multi-layer neural network is also done using gradient descent, it is just that computing the correct partial derivatives for *all* the parameters is a lot more tricky. The algorithm to do this is called *backpropagation* and is something we will look at next module.\n",
    "\n",
    "For this assignment, you're just provided with the already learned weight matrices for this digit recognition problem. Running the code below will load the included matrix files and create a complete 2 layer network in `digitNN`. The network has 64 inputs, 65 hidden nodes, and 3 outputs, with each output corresponding to one of the 3 possible digits; 1, 2 and 3.\n",
    "\n",
    "For this last step, you will need to compute the accuracy of this network in predicting the digits in this data set. The network output will be a $542 \\times 3$ matrix, where the first column indicates the probability that sample was a $1$, the second column the probability the sample was a $2$ and the last column the probability that sample was a $3$. Ultimately, the model should classify each sample as whichever digit was *most likely*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d434692d7b6e8014f23ac8716921973",
     "grade": false,
     "grade_id": "cell-b798b56e722cc19b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 9\n",
    "\n",
    "Implement the function `compute_accuracy`, which takes a matrix of network outputs `Y_hat` and a one-hot encoded set of class labels `Y`, and returns the percentage of samples that was classified correctly by the network.\n",
    "\n",
    "*Hint:* The function [np.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) will save you a lot of work when applied correctly to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f011bbb8dcf426bf44e5c7488282475",
     "grade": true,
     "grade_id": "cell-81121b39559cc407",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " # Normalize the values of the pixels to be between 0 and 1\n",
    "X = digits[:, :-1] / 16\n",
    "\n",
    "# Generate one-hot encoding for Y\n",
    "y = digits[:, -1]\n",
    "Y = np.eye(y.max())[y - y.min()]\n",
    "\n",
    "# Load the already backpropagated weights for the network\n",
    "digitNN = NN((64,65,3))\n",
    "L0 = np.load('data/digits_theta_1.npy')\n",
    "L1 = np.load('data/digits_theta_2.npy')\n",
    "digitNN.layers[0].b = L0[:,0]\n",
    "digitNN.layers[0].W = L0[:,1:]\n",
    "digitNN.layers[1].b = L1[:,0]\n",
    "digitNN.layers[1].W = L1[:,1:]\n",
    "\n",
    "def compute_accuracy(Y_hat, Y):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Compute the network outputs for the digits\n",
    "digit_outputs = digitNN.forward(X)\n",
    "\n",
    "print(\"\\nThe network accuracy for these digits was:\")\n",
    "print(compute_accuracy(digit_outputs, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e94fd767c8adf7cfda4de0a51a10f5cb",
     "grade": false,
     "grade_id": "cell-c7ea7ab2f273a2df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Learning\n",
    "\n",
    "Next week we're going to learn how you can train neural networks in general. But let's glance into the future and look at how we can train **a single** logistic layer. For single layer neural networks, learning the parameters, $W$ and $b$, is very similar to learning $\\theta$ with logistic regression.\n",
    "\n",
    "The single layer network takes a vector of inputs for a sample and produces a vector of predictions (instead of just one prediction per sample, as with logistic regression). This vector of predictions corresponds with multi-class classification, with each output learning a separate *one-vs-all* classifier; one classifier to recognize each of the classes that needs to be distinguished. This means that learning the parameters for the whole neural network will, in the case of a *single layer*, just correspond learning several logistic regression outputs \"next to each other\".\n",
    "\n",
    "#### As a computational graph\n",
    "\n",
    "Just as with logistic regression, we use'll the cross entropy loss $l$. However, since the output of the logistic layer is a vector of predictions, the equation for the cost will also have to change accordingly.  To determine the combined cost for all these one-vs-all classifiers, you can just sum the individual cost for each of the outputs together, and compute the total cost that way.\n",
    "\n",
    "At every training step we would like to calculate how much every weight in $W$ and $b$ should change to decrease this total loss. For this we need to compute the gradients $\\frac{\\partial l}{\\partial W}$ and $\\frac{\\partial l}{\\partial b}$. In the computational graph below we added these gradients:\n",
    "\n",
    "<img src=\"src/Multi layer NN back.svg\" width=\"39%\">\n",
    "\n",
    "#### The math\n",
    "\n",
    "Again, since the output of the logistic layer is not a single value, but a vector, the equations for the gradients are also slightly different to those of logistic regression. If you work out the math, you will see that the difference boils down to doing an extra summation over the outputs. The details are for next week. For now, we will just provide you with the resulting equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{W,b}}{\\partial W} &= \\frac{1}{m}(\\hat{Y} - Y)^T \\cdot X \\\\\n",
    "\\frac{\\partial J_{W,b}}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{Y} - Y)^{\\mathrm{row} = i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial J_{W,b}}{\\partial b}$ is simply the sum of all the rows of the $m \\times o$ matrix $\\hat{Y} - Y$.\n",
    "\n",
    "You can verify for yourself that this is congruent with logistic regression by taking the output dimension $o = 1$ (i.e., you can take $Y$ and $\\hat{Y}$ to be vectors of size $m$ instead of $m \\times o$ matrices). If you work out the math for yourself, you will see that the above equations become identical to those of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94d0ae600cacc61b2ef6889db90e4d7a",
     "grade": false,
     "grade_id": "cell-463e91074827ac8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 10\n",
    "\n",
    "Use the formulas above to define the `LogisticLayer` class here below. The `init` is already given and you can copy the `forward` method from the earlier version of `LogisticLayer` from *Assignment 2*. You'll need to add the missing methods `backward`, `step`, and `cost` to this `LogisticLayer` class.\n",
    "\n",
    "Note that all these functions, so `forward`, `backward`, `step`, and `cost`, should work in exactly the same way as they do for the `Logistic` class from *Assignment 1*, except that they should implement these operations for the entire layer of outputs, instead of a single node. Specifically, each function should be implemented in such a way that they can be used by *same* `optimize` function from earlier, also used for the `Logistic` class, with only the model instance changing. \n",
    "\n",
    "So, most of these functions will need to store some intermediate results in the object instance, which can then be used by later function calls. For example, the `backward` function should store the passed `Y` matrix, making it possible to later compute the `cost` for that same `Y`, without needing to provide it as an argument. Take a good look at what arguments the functions from the `Logistic` class exactly take, and which attributes they store, and use that as a starting point for your `LogisticLayer` implementation.\n",
    "\n",
    "The implementation of `backward` only has to work for a single layer, for which the gradient equations are given above. You can reuse the code from logistic regression from earlier, but care should be taken in the vectorization. Since the there are now $o$ different outputs, the vectorized version will produce an $m \\times o$ matrix of predictions $\\hat{Y}$, which means ($\\hat{Y} - Y$) in the gradient computation is now also an $m \\times o$ matrix, instead of just a vector of size $m$. As a result, you'll need to make some changes in your implementation to keep all of the vectorized dimensions correct.\n",
    "\n",
    "The same applies for the `cost` function, which should now also work for multiple output nodes. This requires an extension of the original logistic regression cost, namely adding an extra sum over all the output nodes, so the resulting cost will still be a single scalar combining all the different outputs. This cost will be introduced in more detail in the theory videos of next week, but can be defined as\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K y^{(i)}_k log(\\hat{y}^{(i)}_k) + (1 - y^{(i)}_k) log(1 - \\hat{y}^{(i)}_k)$$\n",
    "\n",
    "where $K$ is the total number of outputs, $y^{(i)}_k$ is the $k^{th}$ target output of the $i^{th}$ sample, and $\\hat{y}^{(i)}_k$ is the $k^{th}$ predicted output for the $i^{th}$ input sample.\n",
    "\n",
    "As we're now combining multiple output nodes, the implementation of this cost cannot easily be written as a matrix multiplication anymore. So, you should instead write this cost function using elementwise operations, where the elements of both matrices are only multiplied/added/etc when all the indices match (check the *Numpy* notebook if you're unsure how to use these). The result of doing these elementwise operations will then still be an $m \\times k$ matrix, which you can just combine and reduce to a scalar using `np.sum`.\n",
    "\n",
    "> Note that the provided `init` method randomizes the $W$ and $b$ parameters. The reason for this will also be discussed next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a4c955865f9ed9fc0182b7021893d52",
     "grade": true,
     "grade_id": "cell-a50659956d204831",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" set initial weights \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.W = np.random.randn(s_out, s_in)\n",
    "        self.b = np.random.randn(s_out)\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1321071f3965e13631e28a71324d85bd",
     "grade": false,
     "grade_id": "cell-bcd5c2728f128f38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The test below optimizes the network using the function defined in the beginning of this notebook. If you defined the `LogisticLayer` correctly, it should learn the correct weights for the *AND* output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2397ba379c6ca67c7864574a905942d9",
     "grade": true,
     "grade_id": "cell-b892e820789301a4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "testLL = LogisticLayer(2, 1)\n",
    "optimize(testLL, X, andY, 0.1, iterations = 1000)\n",
    "predictions = round_output(testLL.forward(X))\n",
    "print(predictions)\n",
    "\n",
    "np.testing.assert_array_equal(predictions, andY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "509b1e78d0a2206747146c68a676952d",
     "grade": false,
     "grade_id": "cell-e0bec5ce089c7dea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 11\n",
    "\n",
    "Use the logistic layer to learn the correct weights for the *OR* output below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03456dace8104931459428425191f287",
     "grade": true,
     "grade_id": "cell-c9b9b2ac4032cd5b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85cfc9b7a87099743f8eca1cdb59e2e4",
     "grade": true,
     "grade_id": "cell-2f71a408de0eb292",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(predictions, np.array([[0], [1], [1], [1]]))\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c5d8585326552909a756e45f39a8bbc",
     "grade": false,
     "grade_id": "cell-bf1e8722cac532a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 12\n",
    "\n",
    "If the logistic layer is able to learn the correct weights for both the AND and the OR tests, then is this a good indication the implementation is correct. However, these tests still only use a single output ($o=1$), and so, to test the logistic layer fully, we'll also add a two output test ($o=2$).\n",
    "\n",
    "For this we'll simply combine the two tests, and have the first output correspond to an AND of the inputs and the second output to an OR of the inputs. This means the target output $Y$ must now have dimensions $4 \\times 2$, as there are still 4 samples, but now 2 outputs for each sample.\n",
    "\n",
    "Create a target output of the correct dimensions, with the first column corresponding to an AND and the second column to an OR. Next, construct a logistic layer with the correct number of inputs and outputs, and train this layer using the optimize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a51956f01c24fbeb31e35dfb9b97e516",
     "grade": true,
     "grade_id": "cell-4eb809f614643c70",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c60cfc23f984bf357a07cc3749628755",
     "grade": true,
     "grade_id": "cell-abb4b2d1e8f1be4e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(predictions, np.array([[0, 0], [0, 1], [0, 1], [1, 1]]))\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a48ed8fad2cbfac920e8111ce5581b2",
     "grade": false,
     "grade_id": "cell-532ba166f2e990b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this module we've looked at several different ways to modularize logistic regression, combining it into logistic layers with multiple logistic outputs. Then we took these logistic layer modules and stacked them together, connecting the outputs of one layer to the inputs of the next. This created a network that is capable of solving much more complex problems than just a single logistic layer, if we set the weights correclty. Lastly, we looked at a version of a single logistic layer, where the weights could be trained directly using gradient descent.\n",
    "\n",
    "For the next module we will combine all of this, together with a new algorithm called *backpropagation*. This will allow us to create multi-layer neural networks, and train the correct weights for each of these layers. These networks can then learn how to solve more complex problems like the XOR or digit recognition using several logistic layers linked together. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
