{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68ebd7cfb555bcf90d544732faad1067",
     "grade": false,
     "grade_id": "cell-04bf4e28045fa30a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Neural networks and backpropagation\n",
    "\n",
    "In this module you will implement the backpropagation algorithm and use it to train neural networks on a couple of datasets. \n",
    "\n",
    "In the last module you learned how to generalize logistic regression for multiple outputs. Then you learned how to combine these generalized logistic regression units into a neural network. You've used the neural network to simulate logic gates and to recognize handwritten digits. \n",
    "\n",
    "In the last part of the module you saw how to train a single generalized logistic unit, which was very similar to training with logistics regression. The main part that you didn't implement was the training of a full neural network. This is the subject of this module. \n",
    "\n",
    "This requires two adaptations to what we've done last time. First we will need to revisit the equations for computing the gradients. It turns out that they are slightly different from what we saw last module. Second, you'll have to implement the backpropagation algorithm, that propagates the gradients back through the neural network.\n",
    "\n",
    "This module consists of four parts:\n",
    "\n",
    "0. We will start with a brief recap of the elements from last module on which we're going to continue to build. \n",
    "1. First you will adapt the LogisticLayer-class to use the revised equations for a logistic layer of a neural network.\n",
    "2. Then you will extend the NN-class to implement the backpropagation algorithm.\n",
    "3. Then you will train different neural networks on several datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f386f2556d23bbb97cecfc0c55e4ccac",
     "grade": false,
     "grade_id": "cell-f6f759b67bb03648",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 0: Recap\n",
    "\n",
    "Let's start by looking back at last module. Last module discussed some concepts that we will expand on in this module.\n",
    "\n",
    "### Generalized logistic regression\n",
    "\n",
    "Last module we saw how neural networks can be built out of logistic regression modules. We called such a module a `LogisticLayer`. \n",
    "\n",
    "#### Computational graph\n",
    "\n",
    "You saw how to represent the logistic layer as a computational graph. This graph is a convenient visualization of the mathematics that make up a logistic layer.\n",
    "\n",
    "<img src=\"src/cg08_2.svg\" width=\"30%\">\n",
    "\n",
    "#### Forward \n",
    "\n",
    "\n",
    "The method `forward` of `LogisticLayer` implemented the prediction step using the same maths as for logistic regression, but generalized for more than one output:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z = A \\cdot W^T \\oplus b &&\n",
    "A_{\\mathrm{next}} = g(Z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Backward\n",
    "\n",
    "The method `backward` implemented the computation of the gradients of the `LogisticLayer`, using the equations below. **These are only correct for a single layer network!**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{1}{m}(\\hat{Y} - Y)^T \\cdot X &&\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{Y} - Y)^{\\mathrm{row} = i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where, since there is only one layer, $X = A$ and $\\hat{Y} = A_{\\mathrm{next}}$. This module we will expand this set of equations to also work in the case of multi-layer networks.\n",
    "\n",
    "> Note that the computational graphs are often defined for single samples (i.e., lowercase `x`) whereas the equations are all given for multiple samples (i.e., uppercase `X`). This is to, on the one hand, conform to the description of computational graphs in descriptions of neural networks that you'll find elsewhere and, on the other hand, to provide you with equations that you can directly implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8588ad6dc11861f58dced361991a0481",
     "grade": false,
     "grade_id": "cell-3fee96a8fab78f2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Neural network (without learning)\n",
    "\n",
    "The class `NN` implemented a fully connected neural network. We worked out the forward pass for a neural network. We did not implement the backward pass for a multi-layer network yet. That will be the task for this module.\n",
    "\n",
    "The class `NN` used a series of `LogisticLayer` modules to build up the network. This way the `NN` class itself contains very little mathematics, most of that is delegated to the `LogisticLayer` class. Providing a clean, modular design.\n",
    "\n",
    "#### Computational graph\n",
    "\n",
    "The computational graph of the forward pass of the $L$ layer neural network shows how the neural network is essentially $L$ logistic layers chained together. The output of logistic layer $i$ is the input of logistic layer $i+1$, and so the final output of layer $L$ depends on all the weight matrices $W_{1}$ to $W_{L}$ and all the bias terms $b_1$ to $b_L$ preceding it.\n",
    "\n",
    "<img src=\"src/cg07.svg\" width=\"100%\">\n",
    "\n",
    "\n",
    "#### Forward\n",
    "\n",
    "We implemented the `forward` method by iterating over all the logistic layers, calling the `forward` method of those and propagating the output forward.\n",
    "\n",
    "#### Loss\n",
    "\n",
    "The only mathematics that is not delegated to the logistic layers, but is implemented in the `NN` class directly, is that of the loss. The loss $l$ is the total cross entropy for each prediction $\\hat{y}^i_k$ with their corresponding target $y^i_k$, summed together\n",
    "\n",
    "$$\n",
    "l = \\sum_{i=1}^m \\sum_{k=1}^{K} - y^i_k \\cdot \\ln(\\hat{y}^i_k) - (1 - y^i_k) \\cdot \\ln(1 - \\hat{y}^i_k)\n",
    "$$\n",
    "\n",
    "Here $m$ is the number of samples evaluated and $K$ is the number of output nodes, which corresponds to the number different classes being predicted by the network. The cost is just the loss normalized by the amount of samples:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{m} l\n",
    "$$\n",
    "\n",
    "#### Backward\n",
    "\n",
    "The training of multi-layer neural networks is the main subject of this module. Last module we ended with the training of only a single logistic layer. Training a whole network is slightly more complex and requires us to implement backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7849279410b3b6fde0b9543cc174d91",
     "grade": false,
     "grade_id": "cell-b3a26924e5a2c588",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Revised equations\n",
    "\n",
    "So what changes when we want to train a fully connected neural network? As mentioned above, we need to change the equations for computing the gradients and we need to implement backpropagation. In this part we will implement and test the new equations as part of the `LogisticLayer` class. In the next part we will implement the backpropagation algorithm as part of the `NN` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3189798ff0ba4eed5f283f6a6ba34cab",
     "grade": false,
     "grade_id": "cell-c603a536c865cc9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Full computational graph of neural network\n",
    "\n",
    "When we add the gradients to the computational graph we get the following:\n",
    "\n",
    "<img src=\"src/cg10.svg\" width=\"100%\">\n",
    "\n",
    "What we want to compute in the backward step is the gradient for each $W_i$ and $b_i$ in the network. The difficulty is that these gradients cannot be computed in isolation. It turns out that, when you work out the mathematics, the gradients in layer $i$ are dependent on the gradient of the next layer ($i+1$). In other words, in order to be able to compute $\\frac{\\partial l}{\\partial W_1}$ and $\\frac{\\partial l}{\\partial b_1}$, we need to first compute $\\frac{\\partial l}{\\partial A_2}$, and in order to compute $\\frac{\\partial l}{\\partial A_2}$ (and $\\frac{\\partial l}{\\partial W_2}$ and $\\frac{\\partial l}{\\partial b_2}$) we need to first compute $\\frac{\\partial l}{\\partial A_3}$, etc. until we reach $\\frac{\\partial l}{\\partial A_L} = \\frac{\\partial l}{\\partial \\hat{Y}}$, which we can compute directly.\n",
    "\n",
    "So, and this is crucial, we need to compute the gradient of the final (output) layer first and then _work our way backwards_ through the graph. Hence, the term _backpropagation_. This is also indicated by the direction of the arrow of the gradients. Note that, for most the backpropagation, we are actually working with the gradients of the loss $l$. Only as a last step will these get combined into the gradients for the cost $J$, when computing $\\frac{\\partial J}{\\partial W_i}$ and $\\frac{\\partial J}{\\partial b_i}$. The backpropagation algorithm will be discussed more formally later, but let's already have a look at the order of computations to get an intuition:\n",
    "\n",
    "* The first thing we need to compute is the gradient for the output predictions: $\\frac{\\partial l}{\\hat{Y}}$. \n",
    "* With that we can compute the gradients in the final layer: $\\frac{\\partial l}{\\partial W_L}$, $\\frac{\\partial l}{\\partial b_L}$, and $\\frac{\\partial l}{\\partial A_L}$. \n",
    "* This gradient $\\frac{\\partial l}{\\partial A_L}$ is then used to compute the gradients $\\frac{\\partial l}{\\partial W_{L-1}}$, $\\frac{\\partial l}{\\partial b_{L-1}}$, and $\\frac{\\partial l}{\\partial A_{L-1}}$. \n",
    "* Then the gradient $\\frac{\\partial l}{\\partial A_{L-1}}$ is used to compute the gradients of the layer before that, etc., _propagating_ gradients all the way _back_ through the network until we reach the first layer.\n",
    "\n",
    "#### Full computational graph of single layer\n",
    "\n",
    "If we zoom in on a single layer we get the computational graph below. In the forward step we have the value $A$ as input and we compute the value for $A_{next}$. In the backward step we get the gradient $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$ as input and we need to determine the gradients $\\frac{\\partial l}{\\partial W}$, $\\frac{\\partial l}{\\partial b}$, and (crucially for the backpropagation to work) $\\frac{\\partial l}{\\partial A}$.\n",
    "\n",
    "<img src=\"src/cg12.svg\" width=\"30%\">\n",
    "\n",
    "> Note that for the final layer, $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$ is equal to $\\frac{\\partial l}{\\partial \\hat{Y}}$. For all the layers before that, $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$ is the gradient computed in the following layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4341a438cae9b15bddea632689b1bcc",
     "grade": false,
     "grade_id": "cell-d7cbf21666190ced",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Gradients\n",
    "\n",
    "So how do we concretely compute these gradients? Here you will see that the equations differ slightly from what you've seen before. Of course, there is the gradient $\\frac{\\partial l}{\\partial A}$, which we never had to compute before. But, also the gradients $\\frac{\\partial l}{\\partial W}$ and $\\frac{\\partial l}{\\partial b}$ are computed slightly differently. They are given by the following equations (with $D$ defined below):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = D^T \\cdot A &&\n",
    "\\frac{\\partial l}{\\partial b} = \\sum_i^m D^{\\mathrm{row}=i} &&\n",
    "\\frac{\\partial l}{\\partial A} = D \\cdot W\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $D$ is a common term in all the gradients given by the following equation:\n",
    "\n",
    "$$\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\frac{\\partial l}{\\partial A_{\\mathrm{next}}}\n",
    "$$\n",
    "\n",
    "Again, the $\\odot$ and $\\ominus$ are not linear algebra operations. They do correspond to simple multiplication and subtraction in Numpy ($*$ and $-$). You could also work out the $D$ term in proper linear algebra operations, but it wouldn't be very helpful as you would have to translate them back into these Numpy operations when implementing the equation for $D$.\n",
    "\n",
    "Note that the gradient $\\frac{\\partial l}{\\partial A}$ is actually a matrix with the same dimensions as $A$, where each element indicates how the activation of that particular node for that particular sample influences the total loss. Only when computing $\\frac{\\partial l}{\\partial b}$ and $\\frac{\\partial l}{\\partial W}$ do the effects of the different samples get summed together, either with an actual summation in the case of $\\frac{\\partial l}{\\partial b}$, or as part of the matrix multiplication in the case of $\\frac{\\partial l}{\\partial W}$. In order to then compute the actual gradient of the cost, these loss gradients still need to be normalized in the same way as the cost, so the final step is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{1}{m} \\frac{\\partial l}{\\partial W} &&\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\frac{\\partial l}{\\partial b}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4e42cdfed1378c2831eddd760628523",
     "grade": false,
     "grade_id": "cell-6079a46c74f58992",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementation of LogisticLayer\n",
    "\n",
    "We already implemented the `LogisticLayer` class in the previous module. However, we implemented the backward step using the simplified (old) equations, since we knew that the layer was always the output layer (it was the only layer). Now we need to adapt the `backward` method of the `LogisticLayer` class to use the new equations instead, as the new equations work for all layers, not only for a single layer as the old equations did. So, repeated from above, this is the computational graph that the `LogisticLayer` class implements:\n",
    "\n",
    "<img src=\"src/cg12.svg\" width=\"30%\">\n",
    "\n",
    "The `forward` method is already implemented for you. They use the same equations as you used in the previous module: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z = A \\cdot W^T \\oplus b &&\n",
    "A_{\\mathrm{next}} = g(Z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You still need to update the `backward` method to compute the gradients using the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{1}{m} D^T \\cdot A &&\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_i^m D^{\\mathrm{row}=i} &&\n",
    "\\frac{\\partial l}{\\partial A} = D \\cdot W &&\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\frac{\\partial l}{\\partial A_{\\mathrm{next}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It should be noted that we're getting a bit ahead of ourselves: The computation of $\\frac{\\partial l}{\\partial A}$ is already part of the implementation backpropagation, but since it must be computed in the `LogisticLayer` class, it makes sense to deal with it now.\n",
    "\n",
    "### Assignment 1a\n",
    "\n",
    "Implement the `backward` method of the `LogisticLayer` class below. Bear in mind:\n",
    "* The input `DA_next` corresponds to $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$\n",
    "* `self.DW` and `self.Db` correspond to $\\frac{\\partial J}{\\partial W}$ and $\\frac{\\partial J}{\\partial b}$, respectively.\n",
    "* `self.DA` corresponds to $\\frac{\\partial l}{\\partial A}$. \n",
    "* The method should return `self.DA`. As, we will see later, the backpropagation algorithm will rely on this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7845aecdef9e51dc876c30769f5a4ede",
     "grade": false,
     "grade_id": "cell-71cfbf72dd772d77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from notebook_checker import start_checks\n",
    "# Start automatic globals checks\n",
    "%start_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0b8d8b76f620dd1457054739e81fa2c",
     "grade": true,
     "grade_id": "cell-3e5e776052f00837",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.seterr(all='raise')\n",
    "\n",
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" set initial weights \"\"\"        \n",
    "        # init the learning parameters\n",
    "        self.W = np.random.randn(s_out, s_in)*0.6 - 0.3\n",
    "        self.b = np.random.randn(s_out)*0.6 - 0.3\n",
    "    \n",
    "        # init the gradients (later computed in backward)\n",
    "        # the gradient of W\n",
    "        self.DW = np.zeros(self.W.shape)\n",
    "        \n",
    "        # the gradient of b\n",
    "        self.Db = np.zeros(self.b.shape)\n",
    "        \n",
    "        # the gradient of input A (for backpropagation)\n",
    "        self.DA = None\n",
    "    \n",
    "    \n",
    "    def manually_set_weights(self, W, b):\n",
    "        \"\"\" Set weights manually. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert self.W.shape == W.shape, \"W: wrong shape\"\n",
    "        assert self.b.shape == b.shape, \"b: wrong shape\"\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\" computes the ouput for this layer (A_next) \n",
    "            based on input A, and parameters W and b. \"\"\"\n",
    "        self.A = A\n",
    "        self.Z = np.matmul(A, self.W.T) + self.b\n",
    "        \n",
    "        self.A_next = 1 / (1 + np.exp(-self.Z) + 1e-10)\n",
    "        return self.A_next\n",
    "    \n",
    "    \n",
    "    def backward(self, DA_next):\n",
    "        \"\"\" compute the layer gradients based on the gradient w.r.t.\n",
    "            the next layer activation (DA_next), and the input (A)\n",
    "            and corresponding output (A_next) as computed in the\n",
    "            prediction step (forward). The method returns DA (the\n",
    "            gradient w.r.t. A). \"\"\"\n",
    "        self.DA_next = DA_next\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    def step(self, alpha = 0.1):\n",
    "        \"\"\" update the learning parameters W and b based on the \n",
    "            gradients (DW and Db) as computed in the backward step \"\"\"\n",
    "        self.b -= alpha * self.Db\n",
    "        self.W -= alpha * self.DW\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a6e9a6bb5d1c7b3fc04d332db970c7c",
     "grade": false,
     "grade_id": "cell-2674e78df2a1ec50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Testing if machine learning algorithms are implemented correctly can be difficult, as many smaller computations tend to form the final prediction together, and thus it can be hard to properly check or even interpret an individual step. This goes doubly so for neural networks, which can be particulary opaque. Even in this single LogisticLayer implementation we can already see some of the reasons why it might be hard to check these implementations, which we'll investigate here.\n",
    "\n",
    "The LogisticLayer consists of a (potentially very large) matrix of weights, and checking a single Logistic output for this by hand is already a very tedious process. Checking all of the individual learned weights are \"correct\" after many iterations of gradient descent using a data set with several training examples is usually completely infeasible, due to the huge number of different computation steps involved.\n",
    "\n",
    "In addition, each of these weights will have different random starting values before learning. This means that each time you run gradient descent, even when using exactly the same training data, the sequence of steps the algorithm takes to reach a solution will be different! *This means that the any tests you write will have to be independent of the specific starting values of the weights.*\n",
    "\n",
    "> **Sidenote:** For a single Logistic Regression layer it is not not strictly necessary that the weights for gradient descent should start randomly. You've even implemented a version in module 1 that would always start with a vector of zeroes. This was possible, and still is, because in logistic regression (and linear regression for that matter) the cost as function of the model parameters is provably convex, irrespective of the specific data set used. The reason for that is far beyond the scope of this course, but what it means is that we can always assume there will be exactly 1 global minimum and so when the algorithm converges, it will have found *the* globally optimal solution. This is **not** the case when using multiple layers in a neural network, which can have several different local minima, some of which may not be as good as the overal global minimum of the cost function. As we will require random starting values later when building the network from these layers, the random values have already been added here, even though any fixed starting value would also work.\n",
    "\n",
    "Designing tests for these large computation networks that do not rely on a specific set of starting weights, but still produces a predictable output every single time can be quite a challenge. Ways to approach this challenge are exactly what the next part of this assignment is about.\n",
    "\n",
    "### Assignment 1b:\n",
    "\n",
    "Test the logistic layer. Try to think of ways to test the logistic layer yourself. What output do you expect for what input?\n",
    "\n",
    "For example, if the dimensions of the gradient $\\frac{\\partial l}{\\partial A}$ should be the same as the dimensions of the input $A$. The same is true for the dimensions of $\\frac{\\partial J}{\\partial W}$ and $W$ and for the dimensions of $\\frac{\\partial J}{\\partial b}$ and $b$. You can test if this is the case.\n",
    "\n",
    "Another example: if $\\frac{\\partial l}{\\partial A_{next}}$ contains only 0's, it means that the prediction is identical to the target. In such a case we should be at a minimum and, if we implemented the model correctly, all other gradients should also be (close to) 0.\n",
    "\n",
    "Discuss with your fellow students what other ways you could test if your implementation is correct, and write at least two tests down below.\n",
    "\n",
    "*Note:* Implementing the suggested tests from the text above would be sufficient for this assignment. However, trying to think of your own tests can be a very useful exercise, so you are encouraged to do so and discuss your ideas with your fellow students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f17f83e11a2926280c05024b32524e57",
     "grade": true,
     "grade_id": "cell-af6223b91c2a68e9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03bbc2584dd03aa6bc88295229f7cdec",
     "grade": false,
     "grade_id": "cell-5430d76774af0486",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q1. Explain why you chose these particular tests and for each test why they have that expected output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddfdf7e04b0120352db0bec55bc857dc",
     "grade": true,
     "grade_id": "cell-9dfc1093206e560a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92685bedbc25f7658d9e63303d436b9c",
     "grade": false,
     "grade_id": "cell-f798180659494896",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After testing the LogisticLayer class yourself, use the test below to see if you get the same answers as us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71b1239305280c5b3e1252c2e9a36540",
     "grade": true,
     "grade_id": "cell-8287c9c8b9ce3175",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Test LogisticLayer: \", end = '')\n",
    "\n",
    "ll = LogisticLayer(3,2)\n",
    "ll.W = np.array([[0.1, 0.2, 0.3], [0.4,0.5,0.6]])\n",
    "ll.b = np.array([0.7,0.7])\n",
    "prediction = ll.forward(np.array([[0.9, 1.0, 1.1], [1.2, 1.3, 1.4]]))\n",
    "gradient = ll.backward(np.array([[0.1, 0.2], [0.3, 0.4]]))\n",
    "\n",
    "_prediction, _gradient = [[0.789, 0.902], [0.818, 0.935]], [[0.009, 0.012, 0.016], [0.014, 0.021, 0.028]]\n",
    "_DW, _Db = [[0.034, 0.037, 0.040], [0.022, 0.025, 0.027]], [0.031, 0.021]\n",
    "\n",
    "np.testing.assert_almost_equal(prediction, _prediction, decimal=3)\n",
    "np.testing.assert_almost_equal(gradient, _gradient, decimal=3)\n",
    "np.testing.assert_almost_equal(ll.DW, _DW, decimal=3)\n",
    "np.testing.assert_almost_equal(ll.Db, _Db, decimal=3)\n",
    "\n",
    "\n",
    "print(\"succes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc4a51aaa5ea2acd208b4639456ff0a0",
     "grade": false,
     "grade_id": "cell-fadc2888c87c444e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Backpropagation\n",
    "\n",
    "With the `LogisticLayer` in place, we're almost there. We now need to connect them all up again into a neural network as we did in last module. But, this time we're going to also do the backward step. Have another look at the computational graph:\n",
    "\n",
    "<img src=\"src/cg10.svg\" width=\"100%\">\n",
    "\n",
    "We've implemented everything that's inside the green rectangles. Now we can focus on the transitions between them. First we need to compute the gradient of the output, $\\frac{\\partial l}{\\partial \\hat{Y}}$. And then we can use the backpropagation algorithm to propagate the gradients $\\frac{\\partial l}{\\partial A_{i}}$ back through the network. \n",
    "\n",
    "First we will look at the $\\frac{\\partial l}{\\partial \\hat{Y}}$, then the backpropagation algorithm.\n",
    "\n",
    "#### The gradient of the output\n",
    "\n",
    "So for backpropagation to work, the first thing we need to determine is the gradient for the output $Y$. For the output of a single sample ($\\hat{y}$), the loss $l$ is the same cross entropy we've seen before:\n",
    "\n",
    "$$\n",
    "l = \\sum_{i=1}^m \\sum_{k=1}^{K} - y^i_k \\cdot \\ln(\\hat{y}^i_k) - (1 - y^i_k) \\cdot \\ln(1 - \\hat{y}^i_k)\n",
    "$$\n",
    "\n",
    "And the gradient for one specific prediction $\\hat{y}^u_v$ for the $u^{th}$ sample and the $v^{th}$ output is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{y}^u_v} = \\frac{\\partial}{\\partial \\hat{y}^u_v} \\left(\\sum_{i=1}^m \\sum_{k=1}^{K} - y^i_k \\cdot \\ln(\\hat{y}^i_k) - (1 - y^i_k) \\cdot \\ln(1 - \\hat{y}^i_k)\\right)\n",
    "$$\n",
    "\n",
    "Which works out to be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{y}^u_v} = \\frac{- y^u_v}{\\hat{y}^u_v} + \\frac{1 - y^u_v}{1 - \\hat{y}^u_v}\n",
    "$$\n",
    "\n",
    "In terms of matrices (multiple samples and outputs) this becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{Y}} = \\ominus Y\\oslash \\hat{Y} + (1\\ominus Y)\\oslash (1\\ominus \\hat{Y})\n",
    "$$\n",
    "\n",
    "Again, the $\\ominus$ and $\\oslash$ are not linear algebra operations (just like $\\odot$, $\\oplus$ and $\\ominus$ before). The operators $\\ominus$ and $\\oslash$ correspond to simple subtraction and division in Numpy ($-$ and $/$). Also here, it wouldn't be helpful to express the $\\frac{\\partial l}{\\partial \\hat{Y}}$ term in true linear algebra, as the current notation is much closer to how you will implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9381cfee3556145e52db9dd7ae0f7b5f",
     "grade": false,
     "grade_id": "cell-59298e8c6756c0e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Backpropagation pseudo code.\n",
    "\n",
    "With the $\\frac{\\partial l}{\\partial \\hat{Y}}$ term defined we can look at the final step: backpropagation. As mentioned earlier, the idea of backpropagation is that you start computing the gradients in the last layer and work your way back through the neural network. With that in mind, have a look at the pseudo code for backpropagation:\n",
    "\n",
    "\n",
    "---\n",
    "1. Run the forward pass and predict: $\\hat{Y}$\n",
    "2. Compute the gradient for the output: $\\frac{\\partial l}{\\partial \\hat{Y}}$\n",
    "3. Set a variable `DA`$:= \\frac{\\partial l}{\\partial \\hat{Y}}$\n",
    "4. Loop for layer in logistic layers i from L to 1 (i.e., backwards)\n",
    "    1. Compute backward pass for logistic layer i (using `layer.backward(DA)`):\n",
    "        1. Compute gradient $\\frac{\\partial J}{\\partial W}$\n",
    "        2. Compute gradient $\\frac{\\partial J}{\\partial b}$\n",
    "        3. Compute gradient $\\frac{\\partial l}{\\partial A}$\n",
    "        4. Return $\\frac{\\partial l}{\\partial A}$\n",
    "    2. Set `DA`$:= \\frac{\\partial l}{\\partial A}$\n",
    "---\n",
    "\n",
    "In this algorithm, steps (a, b, c, and d) are already implemented in the `LogisticLayer` class. So the only thing you have to do is call the `backward()` method of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5d3f4f03f0892485faef3e2e5164e31",
     "grade": false,
     "grade_id": "cell-677b557e29ce6feb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2a\n",
    "\n",
    "The neural network below is mostly implemented. Only the `backward` method, implementing the backpropagation algorithm, is missing. Implement this method. \n",
    "\n",
    "Keep in mind that the only maths that you still have to implement in the `backward` method is that for $\\frac{\\partial l}{\\partial \\hat{Y}}$. The rest of the mathematics is delegated to the `LogisticLayer` objects that are stored in `self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "015bc9509f5385732a0c01da7a03dc1e",
     "grade": true,
     "grade_id": "cell-87c3ec7cd6d6d29a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, layer_sizes = [2,2,1]):\n",
    "        \"\"\" Set initial layers. \"\"\"\n",
    "        self.layers = []\n",
    "        for s_in, s_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(LogisticLayer(s_in, s_out))\n",
    "         \n",
    "    \n",
    "    def manually_set_weights(self, Ws, bs):\n",
    "        \"\"\" Provide list of weight matrices Ws and list of bias vectors bs. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert len(Ws)==len(self.layers), \"Ws: wrong length\"\n",
    "        assert len(bs)==len(self.layers), \"bs: wrong length\"\n",
    "        \n",
    "        for layer, W, b in zip(self.layers, Ws, bs):\n",
    "            layer.manually_set_weights(W, b)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction for X based on self.layers\"\"\"\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        self.Y_hat = A\n",
    "        return self.Y_hat\n",
    "    \n",
    "    \n",
    "    def backward(self, Y):\n",
    "        self.Y = Y\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    def cost(self, Y = None):\n",
    "        if not Y:\n",
    "            Y = self.Y\n",
    "        j = - Y * np.log(self.Y_hat) - (1 - Y) * np.log(1 - self.Y_hat)\n",
    "        return np.sum(j) / len(Y)\n",
    "    \n",
    "    \n",
    "    def step(self, alpha = 0.1):\n",
    "        for layer in self.layers:\n",
    "            layer.step(alpha = alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03cfdaf273a94d66d31fb6c5de292b39",
     "grade": false,
     "grade_id": "cell-b64ba8c731405514",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have a complete neural network class, let's try and design some tests for it. Designing tests for a neural network can even have some additional difficulties compared to just the LogisticLayer, as the random starting weights are now essential for the network to function.\n",
    "\n",
    "The random starting weights ensure that each of the hidden nodes learn different weights (instead of them all recieving the exact same gradient), allowing different nodes to learn different things. This also means we have no control over what exactly the first hidden node learns compared to the second hidden node, and it is even likely to be different each time we run gradient descent. As a result, these weights are very unlikely to ever converge to the same exact value and they might even end up in a completely different (local) minimum when converged.\n",
    "\n",
    "So here the challenge is again to write some sensible tests for a large network of computations, and specifcally to write tests that will always be independent of the starting values of the weights and the exact minimum the network ends up in, as this is *not* guaranteed to be the global minimum.\n",
    "\n",
    "### Assignment 2b\n",
    "\n",
    "Try to think of ways you can test the correctness of the `NN` class yourself. Some ideas to start with: What should be the value of the gradients if the target and predictions are all the same? What should be the dimensions of the output if we define a neural network with `layer_sizes = [3,2,1]` and an input matrix of shape `(18, 3)`? What should the dimensions of all the gradients be?\n",
    "\n",
    "Discuss with your fellow students what other ways you could test if your implementation is correct, and write at least two tests down below.\n",
    "\n",
    "*Note:* Implementing the suggested tests from the text above would be sufficient for this assignment. However, trying to think of your own tests can be a very useful exercise, so you are encouraged to do so and discuss your ideas with your fellow students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1a7627c6948e0147a16ccd229b7fb71",
     "grade": true,
     "grade_id": "cell-a1b400452aa81b8d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b805b28a391fa7f19d72aee9f3b1d72d",
     "grade": false,
     "grade_id": "cell-885a2d8a9086855e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2. Explain why you chose these particular tests and for each test why they have that expected output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18baebf5e9ee4a7f9c539b16f4f66002",
     "grade": true,
     "grade_id": "cell-07236c97850902a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eaebc4ac45fc4674dcc41446c0afa228",
     "grade": false,
     "grade_id": "cell-d285399505bd7325",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After testing the `NN` class yourself, use the test below to see if you get the same answers as us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d75a19892354252026e633b2ebc27467",
     "grade": true,
     "grade_id": "cell-224e3df5eed5a1a4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Test NN: \", end = '')\n",
    "\n",
    "np.random.seed(0)\n",
    "nn = NN([3,2,1])\n",
    "prediction = nn.forward(np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]))\n",
    "target = np.array([[1], [-1]])\n",
    "nn.backward(target)\n",
    "\n",
    "\n",
    "_prediction = [[0.3885], [0.3817]]\n",
    "np.testing.assert_almost_equal(prediction, _prediction, decimal=3)\n",
    "\n",
    "_gradients = [{'DW': [[-0.0194, -0.0223, -0.0251], [-0.0033, -0.0038, -0.0044]], \n",
    "               'Db': [-0.0287, -0.0053], 'DA': [[0.0484, 0.0033, 0.0082], [-0.103, -0.0086, -0.0153]]}, \n",
    "              {'DW': [[0.2795, 0.2055]], 'Db': [0.3851], 'DA': [[0.2213, 0.0328], [-0.5001, -0.0741]]}]\n",
    "for layer, _answer in zip(nn.layers, _gradients):\n",
    "    np.testing.assert_almost_equal(layer.DW, _answer['DW'], decimal=3)\n",
    "    np.testing.assert_almost_equal(layer.Db, _answer['Db'], decimal=3)\n",
    "    np.testing.assert_almost_equal(layer.DA, _answer['DA'], decimal=3)\n",
    "\n",
    "\n",
    "print(\"succes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0967306de0d869d02f7c4cd005ec01fb",
     "grade": false,
     "grade_id": "cell-363c2f5f47eaafdd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Intermezzo: Why did the equations change?\n",
    "\n",
    "This part doesn't contain any assignments, but you should read it as it will help you understand what's going on.\n",
    "\n",
    "You might be confused or surprised by the maths. Why is the math for a multi layer network different from the math of the single layer network that we've seen in the previous module? Of course, for a single layer you don't have to compute the gradient for $A$ since there is no backpropagation, but you would still expect the gradients for $W$ and $b$ to be described by the same equations.\n",
    "\n",
    "However, these are the equations for a single logistic layer, as implemented in the previous module:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{1}{m}(\\hat{Y} - Y)^T \\cdot X && \\frac{\\partial J}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{Y} - Y)^{\\mathrm{row} = i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And these are the equations for a layer in a neural network, as implemented in this module (let's call these the *standard equations*):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{1}{m} D^T \\cdot A &&\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_i^m D^{\\mathrm{row}=i} &&\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\frac{\\partial l}{\\partial A_{\\mathrm{next}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can see that both sets of equations have a similar form, but they are clearly not the same. How come? This is a subtle, but important, point that is often omitted in explanations of neural networks: If we assume that the layer of the network is the final layer, then the equations _are actually equivalent_. \n",
    "\n",
    "Why? If the layer is the last layer, then by definition:\n",
    "\n",
    "$$\n",
    "A_{\\mathrm{next}} = \\hat{Y}\n",
    "$$\n",
    "\n",
    "This changes the last equations of the final layer:\n",
    "\n",
    "$$\n",
    "D = \\hat{Y} \\odot(1 \\ominus \\hat{Y}) \\odot \\frac{\\partial l}{\\partial \\hat{Y}}\n",
    "$$\n",
    "\n",
    "where we can substitute the definition of the derivative of the loss w.r.t. the predicted output:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{Y}} = \\ominus Y\\oslash \\hat{Y} + (1\\ominus Y)\\oslash (1\\ominus \\hat{Y})\n",
    "$$\n",
    "\n",
    "resulting in\n",
    "\n",
    "$$\n",
    "D = \\hat{Y} \\odot(1 \\ominus \\hat{Y}) \\odot [\\ominus Y\\oslash \\hat{Y} + (1\\ominus Y)\\oslash (1\\ominus \\hat{Y})]\n",
    "$$\n",
    "\n",
    "Which, if we work out the math and let the dust settle, reduces to:\n",
    "\n",
    "$$\n",
    "D = \\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "> You can verify this for yourself. Since none of the above operations are linear algebra operations you can translate the equation back to a normal algebra equation:\n",
    "$$\n",
    "d= \\hat{y} \\cdot (1 - \\hat{y}) \\cdot [-y / \\hat{y} + (1 - y) / (1 - \\hat{y})]\n",
    "$$\n",
    "you can work out for yourself that this indeed reduces to:\n",
    "$$\n",
    "d = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "If we now fill in $D = \\hat{Y} - Y$ into the original equation we get:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial W}\\ = \\frac{1}{m} D^T \\cdot A\\ &= \\frac{1}{m} (\\hat{Y} - Y)^T \\cdot A\\\\ \n",
    "\\frac{\\partial J}{\\partial b}\\ = \\frac{1}{m} \\sum_i^m D^{\\mathrm{row}=i}\\ &= \\frac{1}{m} \\sum_i^m (\\hat{Y} - Y)^{\\mathrm{row}=i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Which should look very familiar!\n",
    "\n",
    "So, as we can see the equations for the final layer of a neural network are equivalent to those for the single Logistic Regression module. The problem is that most layers are not the final layer. And for those, this simplification doesn't work. So in general we are stuck with the more complex standard equations.\n",
    "\n",
    "In principle we could make a special case for the final layer. We could define one layer which is a logistic layer combined with the cost function, so we can use the simplified equations for that one layer and use the standard equations for all the other layers. This would be a bit more efficient (saving some costly operations) and it has some technical advantages (saving us from some potential rounding errors). You will see in the future that creating a special final layer is quite common practice, but we do not do this in this module. Using the same (standard) equations for all layers gives us a cleaner implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "701658b5b8727640c3d945ff2f26fb44",
     "grade": false,
     "grade_id": "cell-88ed2e3f8b6a2372",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Training the neural network\n",
    "\n",
    "Now it's time to really put our neural network to the test! First let's start with some functions from previous module. \n",
    "\n",
    "As you might see, the `optimize` is identical to previous module. There we used it for logistic regression, but since all gradient descent algorithms work the same and we've defined the same methods (`forward()`, `backward()`, `step()`, and `cost()`), we can use the exact same function for training a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d620a41ce01ece67cc2351e7e9d79c9c",
     "grade": false,
     "grade_id": "cell-19e1c4ac033f34e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize(model, X, y, alpha, iterations = 500, desc = 'iteration'):\n",
    "    \"\"\"Apply gradient descent to any model that inherits the Module class.\"\"\"\n",
    "    costs = []\n",
    "    for i in tqdm(range(iterations), leave=False, desc = desc):\n",
    "        # descent\n",
    "\n",
    "        model.forward(X)        \n",
    "        model.backward(y)\n",
    "        model.step(alpha)\n",
    "        \n",
    "        # keep track of costs\n",
    "        costs.append(model.cost())\n",
    "        \n",
    "        # check for divergence (alpha too big)\n",
    "        if len(costs) >= 2 and (costs[-2] - costs[-1]) < 0:\n",
    "            print(f'Diverging at iteration {len(costs)}')\n",
    "            return costs  \n",
    "    return costs\n",
    "\n",
    "def plot_costs(costs):\n",
    "    plt.title(\"The development of the cost of the model\")\n",
    "    plt.plot(range(len(costs)), costs)\n",
    "    plt.show()\n",
    "\n",
    "def confusion_matrix(p, y):\n",
    "    return np.matmul(np.vstack((p, 1 - p)), np.vstack((y, 1 - y)).T)\n",
    "\n",
    "def round_output(x):\n",
    "    return (x >= 0.5) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "860ab30c1c23c48ae21dd6ad613acab8",
     "grade": false,
     "grade_id": "cell-7a3b4c6a2fbc54bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### *Case 1: Logic gates*\n",
    "\n",
    "The code below uses your neural network to learn a logic *and*. If you implemented the neural network correctly, it should work as is.\n",
    "\n",
    "### Assignment 3\n",
    "\n",
    "While the code below works, maybe you can get it to learn a bit faster. Play around with the learning rate and try to minimize the number of required iterations. Make sure you still reach a good optimum for the network parameters and don't end up with any false positives or false negatives in your predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "predictions = np.zeros((0,1))\n",
    "target = np.zeros((0,1))\n",
    "\n",
    "## run NN 20 times to make sure it always works (no matter the initialization of the learning parameters)\n",
    "for i in range(20):\n",
    "    testNN = NN([2, 1])\n",
    "    costs = optimize(testNN, X, andY, 0.01, iterations = 10000, desc = f'trial {i:2}')\n",
    "    _predictions = round_output(testNN.forward(X))\n",
    "    \n",
    "    # store results for comparison\n",
    "    predictions = np.append(predictions, _predictions, axis = 0)\n",
    "    target = np.append(target, andY, axis = 0)\n",
    "\n",
    "print(confusion_matrix(predictions[:,0], target[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c999537ea40674402bd45571a95917e4",
     "grade": false,
     "grade_id": "cell-63259be1813b1b59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q3. What is the minimum amount of iterations you needed to correctly learn this data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32a1bee4377e8ef5dbaf9dd84a2e0d05",
     "grade": true,
     "grade_id": "cell-5f2e99ef840c2e0f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0410c67f18fe9912769098d9f060bb2",
     "grade": false,
     "grade_id": "cell-1c9744d4c3a744b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q4. Add a hidden layer of size 2 to the network. Does this change the optimal learning rate and number of iterations? Explain how.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "925d09f2515f35480329ea1d5f2bae7c",
     "grade": true,
     "grade_id": "cell-a36398ec8413de65",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "343bfddf6e4996fbfcae26d8c33cd090",
     "grade": false,
     "grade_id": "cell-0f7245bdc8d23767",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 4\n",
    "\n",
    "The code below should learn a logical *xor*, but it doesn't work. Fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "\n",
    "predictions = np.zeros((0,1))\n",
    "target = np.zeros((0,1))\n",
    "\n",
    "## run NN 20 times to make sure it always works (no matter the initialization of the learning parameters)\n",
    "for i in range(20):\n",
    "    testNN = NN([2, 1])\n",
    "    costs = optimize(testNN, X, xorY, 0.01, iterations = 5000, desc = f'trial {i:2}')\n",
    "\n",
    "    _predictions = round_output(testNN.forward(X))\n",
    "    \n",
    "    # store results for comparison\n",
    "    predictions = np.append(predictions, _predictions, axis = 0)\n",
    "    target = np.append(target, xorY, axis = 0)\n",
    "\n",
    "\n",
    "print(confusion_matrix(predictions[:,0], target[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bc650e36ed2ef6c4c7ba8480fb9345b",
     "grade": false,
     "grade_id": "cell-c44c99dc325dd4cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q5. Explain what you did to fix this, and why that was necessary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14fb87cc8b8e16606f5bacc4bc559fe8",
     "grade": true,
     "grade_id": "cell-880f28c825a0a6e6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12f7dd2ebd7ece84381a50dff51e6b95",
     "grade": false,
     "grade_id": "cell-ea3dc8d29f7872c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### *Case 2: Digits*\n",
    "\n",
    "Last module you got a pre-trained neural network for recognizing handwritten digits 1, 2 and 3. Now it's time to train this ourselves. First, load the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = np.loadtxt('data/digits123.csv', delimiter=',', dtype=int)\n",
    "\n",
    "def display_digit(i, digits):\n",
    "    digit_sample = np.ones((8,8))*16 - np.reshape(digits[i, :-1], (8, 8))\n",
    "    plt.imshow(digit_sample, cmap='gray', vmax=16)\n",
    "    plt.show()\n",
    "    print(\"The label for this digit was:\", digits[i, -1])\n",
    "\n",
    "display_digit(200, digits)\n",
    "\n",
    "# Normalize the values of the pixels to be between 0 and 1\n",
    "X = digits[:, :-1] / 16\n",
    "\n",
    "# Generate one-hot encoding for Y\n",
    "y = digits[:, -1]\n",
    "Y = np.eye(y.max())[y - y.min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7daf17edd7018a1f7a6f04057c39d08",
     "grade": false,
     "grade_id": "cell-42522856d9190ed4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 5\n",
    "\n",
    "Define and train a neural network, called `digit_NN`, for the digit data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c152e8d6d4a8bcaaebd1468be56bd89",
     "grade": true,
     "grade_id": "cell-a44e5c4855083891",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, test_size = 0.3,  random_state=11)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def compute_accuracy(H, Y):\n",
    "    return ((np.argmax(H, axis = 1) == np.argmax(Y, axis = 1))*1).mean()\n",
    "\n",
    "# Compute the network outputs for the digits\n",
    "digit_outputs = digitNN.forward(X_test)\n",
    "\n",
    "print(\"\\nThe test set accuracy for these digits was:\")\n",
    "print(compute_accuracy(digit_outputs, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4e181797020c4d0311d7fc8744b57f1",
     "grade": false,
     "grade_id": "cell-0bd130a638e0d6e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q6. Which configuration of layers gave you the best results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa71e614e44da6283c05ae117c59b6b7",
     "grade": true,
     "grade_id": "cell-7ba267ca79b136c5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab934625cd10c7b3204e679acf2e73d4",
     "grade": false,
     "grade_id": "cell-1eebb2263865eebb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### *Case 3: Iris*\n",
    "\n",
    "As a last use-case, let's look at data you have seen before in a different context. You have used the Iris dataset in an unsupervised learning context. But you can also use supervised learning to achieve something similar. The Iris samples are categorized into three classes. We can use a neural network to classify the data based on those classes. Load the Iris data below, read the description of the data set, and check what the features and labeled classes correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ecca19b3ee19e4d324800c403e12026",
     "grade": false,
     "grade_id": "cell-c6dd117f7ea1b59d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_raw = iris.data\n",
    "y_raw = iris.target\n",
    "\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "756dec2b5f11e1e60e58aaece7acad16",
     "grade": false,
     "grade_id": "cell-1099b7da8ee29ee9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6a \n",
    "\n",
    "The data should be modified so that it can be used with a neural network.\n",
    "\n",
    "First, you should normalize `X_raw`, i.e. make sure that all features are converted to values between 0 and 1. Store the result in a variable `X`.\n",
    "\n",
    "*Hint:* Make sure to normalize each feature separately. What *Numpy* function could you use to compute all normalization constants at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0f9e89680b13b4eb6389d7923709de6",
     "grade": true,
     "grade_id": "cell-67f314cb7b1f84f5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b78a62c194e8601a23e84ceb17d032c3",
     "grade": false,
     "grade_id": "cell-a584c8607450c073",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6b\n",
    "\n",
    "Second, the variable `y_raw` contains class data (values 0, 1, or 2). Transform this to one-hot encoding and store the result in a variable `Y`. \n",
    "\n",
    "*Hint:* Making good use of existing *Numpy* functions will save some time here. Specifically the [eye](https://numpy.org/doc/stable/reference/generated/numpy.eye.html) function and [index arrays](https://numpy.org/doc/stable/user/basics.indexing.html#index-arrays) might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f269b0acf588d9e4db23ca1b396ec3",
     "grade": true,
     "grade_id": "cell-b4f280c70a53f8d2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11969550da928466182d562209137229",
     "grade": false,
     "grade_id": "cell-71344bd658cc6fa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6c\n",
    "\n",
    "Now split the data in a train and test set. Make sure that the train size is 70% and the test size is 30% of the data. You may use library functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7126bb4d6909e43de917177482334a09",
     "grade": true,
     "grade_id": "cell-d618314b68bcf855",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b638445980d6f4e0d4f994f5466b034",
     "grade": false,
     "grade_id": "cell-6c31f7982eae45c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6d\n",
    "\n",
    "Finally create a neural network, train it on the train data, and compute the accuracy with the test data. It should be possible to get an accuracy above 90%.\n",
    "\n",
    "You can use the function `plot_costs` to monitor the cost during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f9b3cc3d94e4b803d56e303491147c9",
     "grade": true,
     "grade_id": "cell-e7bce2c764dc21ac",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "680af5472d64415b1eb9cd74cb998d32",
     "grade": false,
     "grade_id": "cell-841f78266d86af85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Final thoughts\n",
    "\n",
    "Done! Let's have a final look at what you did. You've implemented `LogisticLayer` and `NN`. You essentially created a modular way for composing neural networks. Next module you will even expand more on this modularity.\n",
    "\n",
    "We gave you the equations for working out the gradients. And you might be wondering where they came from. One of the subtle points that you might not have noticed is that this modular computation of the gradients is done by relying heavily on the application of the chain rule. Let's consider this with a 2 layer network using the non-vectorized maths ($w_1$ and $b_1$ are just scalar parameters). Then, the network is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_1 = x &&\n",
    "z_1 = w_1a_1+b_1 &&\n",
    "a_2 = g(z_1) &&\n",
    "z_2 = w_2a_2+b_2 &&\n",
    "a_3 = g(z_2) &&\n",
    "\\hat{y} = a_3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "l = -y\\ln(\\hat{y}) - (1 - y)\\ln(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "The loss of this network $l$ is the result of applying quite a few equations in order. Each of these equations can be seen as a function, and so, in order to compute $\\frac{\\partial l}{\\partial w_1}$, we can use many applications of the chain rule to get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_1} = \\frac{\\partial l}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "In principle you could compute this whole derivative as one single expression, but it is already impractical and you can imagine how unwieldy this would get if you increase the number of layers. What we did is break it up. During the backpropagation every logistic layer computes the term $\\frac{\\partial l}{\\partial a}$ for the previous layer. This no longer has to be worked out as part of the expression. So, starting with the last layer, the partial derivative w.r.t. the activation $a_3$ can be computed and the expression simplified: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial a_3}}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial a_3}}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "Then we can use the computed $\\frac{\\partial l}{\\partial a_3}$ and the activations at $a_2$ to further compute partial derivative w.r.t. $a_2$, resulting in:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial a_2}}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "The newly computed $\\frac{\\partial l}{\\partial a_2}$ can then be used directly to compute the partial derivative w.r.t. $w_1$. The computation of the term $\\frac{\\partial l}{\\partial a}$ at every layer is thus at the core of the backpropagation algorithm.\n",
    "\n",
    "Returning to the full vectorized equations from the assignment, the equations that you got for computing $\\frac{\\partial l}{\\partial W}$ were:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = D^T \\cdot A &&\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\boxed{\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "are a vectorized version of working out:\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w} = \\boxed{\\frac{\\partial l}{\\partial a_\\mathrm{next}}}\\frac{\\partial a_\\mathrm{next}}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "So you've essentially been applying the chain rule at every step of the backpropagation algorithm.\n",
    "\n",
    "You now understand the basis of any neural network. From here on you will learn tricks to speed up the learning, avoid local minimums, avoid overfitting, and other improvements, but you now know the foundations on which those are built."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
